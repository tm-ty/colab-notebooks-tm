# -*- coding: utf-8 -*-
"""Crawler Manual (Crawl4AI) v3.1 - Working (helper Gemini exp 1206).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cB_NTB3t1wIS3EyXdek0SflBjt14pzqo

# v1

## passo 1
"""

!pip install -U crawl4ai

!crawl4ai-setup

!crawl4ai-doctor

!python -m playwright install --with-deps chromium

"""## Passo 2: Adaptar o c√≥digo"""

# OBS.: As duas linhas baixo n√£o s√£o necess√°rias no Colab
# parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# sys.path.append(parent_dir)

"""Passo 3: Implantar a solu√ß√£o"""

from google.colab import drive
drive.mount('/content/drive/')

"""## Passo 4: Monitorar e obter resultados"""

# Instalar depend√™ncias
!pip install -U crawl4ai
!crawl4ai-setup
!crawl4ai-doctor
!python -m playwright install --with-deps chromium

# Importar bibliotecas
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

# Fun√ß√£o para rastreamento sequencial
async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")

    browser_config = BrowserConfig(
        headless=True,
        # For better performance in Docker or low-memory environments:
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )

    # Criar o crawler (abre o navegador)
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "session1"  # Reutilizar a mesma sess√£o em todas as URLs
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")
                # Exemplo: verificar o tamanho do markdown
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")
            else:
                print(f"Failed: {url} - Error: {result.error_message}")
    finally:
        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)
        await crawler.close()

# Fun√ß√£o principal
async def main():
    urls = [
        "https://ai.pydantic.dev/agents/",
        "https://ai.pydantic.dev/models/",
        "https://ai.pydantic.dev/dependencies/",
        "https://ai.pydantic.dev/tools/"
    ]
    await crawl_sequential(urls)

# Executar a fun√ß√£o principal
#asyncio.run(main()) # <-- LINHA que poss√≠velmente deu erro
await main()

!ls

!ls /content/drive/MyDrive/'Projetos Python - Google Colab (Colab Notebooks)'/Crawl4AI/Pydantic

!zip -r /content/arquivos.zip /content/drive/MyDrive/pasta_com_seus_arquivos

"""My Drive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic"""



"""# v2 - Modifica√ß√£o para baixar .md no diret√≥rio "/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic"
"""

!pip install -U crawl4ai

# Instalar depend√™ncias
!pip install -U crawl4ai
!crawl4ai-setup
!crawl4ai-doctor
!python -m playwright install --with-deps chromium

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importar bibliotecas
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

# Definir o diret√≥rio de sa√≠da
output_directory = "/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic"

# Fun√ß√£o para rastreamento sequencial
async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")

    browser_config = BrowserConfig(
        headless=True,
        # For better performance in Docker or low-memory environments:
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator(),
        output_dir=output_directory  # Configurar o diret√≥rio de sa√≠da aqui
    )

    # Criar o crawler (abre o navegador)
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "session1"  # Reutilizar a mesma sess√£o em todas as URLs
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")
                # Exemplo: verificar o tamanho do markdown
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")
            else:
                print(f"Failed: {url} - Error: {result.error_message}")
    finally:
        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)
        await crawler.close()

# Fun√ß√£o principal
async def main():
    urls = [
        "https://ai.pydantic.dev/agents/",
        "https://ai.pydantic.dev/models/",
        "https://ai.pydantic.dev/dependencies/",
        "https://ai.pydantic.dev/tools/"
    ]
    await crawl_sequential(urls)

# Executar a fun√ß√£o principal (usando await no Colab)
await main()

# Instalar depend√™ncias
!pip install -U crawl4ai
!crawl4ai-setup
!crawl4ai-doctor
!python -m playwright install --with-deps chromium

# Montar o Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Importar bibliotecas
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

# Definir o diret√≥rio de sa√≠da
output_directory = "/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic"

# Fun√ß√£o para rastreamento sequencial (modificada)
async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")

    browser_config = BrowserConfig(
        headless=True,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
        # Remova output_dir daqui
    )

    # Criar o crawler (abre o navegador)
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "session1"  # Reutilizar a mesma sess√£o em todas as URLs
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")

                # Salvar o arquivo manualmente
                filename = os.path.join(output_directory, f"{url.replace('/', '_')}.md") # Nome do arquivo baseado na URL
                os.makedirs(os.path.dirname(filename), exist_ok=True) # Cria o diret√≥rio se necess√°rio
                with open(filename, "w") as f:
                    f.write(result.markdown_v2.raw_markdown)
                print(f"Markdown saved to: {filename}")
            else:
                print(f"Failed: {url} - Error: {result.error_message}")
    finally:
        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)
        await crawler.close()

# Fun√ß√£o principal
async def main():
    urls = [
        "https://ai.pydantic.dev/agents/",
        "https://ai.pydantic.dev/models/",
        "https://ai.pydantic.dev/dependencies/",
        "https://ai.pydantic.dev/tools/"
    ]
    await crawl_sequential(urls)

# Executar a fun√ß√£o principal (usando await no Colab)
await main()

"""# V3 - üî• Faz o Crawler e baixa diretamente no Desktop üî•
OBS.: Eu estava colocando pra salvar no Drive mas a a vers√£o do crawl4ai (0.4.247) que voc√™ est√° usando n√£o suporta o argumento output_dir no construtor CrawlerRunConfig.

**Explica√ß√µes:**

1. **`temp_output_directory = "/content/crawl4ai_temp_output"`:** Define um diret√≥rio tempor√°rio dentro do ambiente do Colab para armazenar os arquivos Markdown.
2. **`crawl_sequential` (modificada):**
    *   O `output_dir` foi removido do `CrawlerRunConfig`.
    *   Os arquivos s√£o salvos temporariamente no `temp_output_directory` usando a l√≥gica que discutimos anteriormente (`os.makedirs`, `with open...`).
3. **`shutil.make_archive("crawl4ai_output", 'zip', temp_output_directory)`:**
    *   Ap√≥s o rastreamento, esta linha usa a fun√ß√£o `make_archive` do m√≥dulo `shutil` para compactar o diret√≥rio tempor√°rio em um arquivo chamado `crawl4ai_output.zip`.
4. **`files.download("crawl4ai_output.zip")`:** Esta linha usa a fun√ß√£o `download` do `google.colab.files` para disponibilizar o arquivo zipado para download no seu navegador.

**Como executar:**

1. Execute o c√≥digo.
2. O c√≥digo ir√° rastrear as URLs e salvar os arquivos Markdown temporariamente.
3. Ao final da execu√ß√£o, o arquivo `crawl4ai_output.zip` ser√° baixado automaticamente pelo seu navegador.
4. Descompacte o arquivo `.zip` para acessar os arquivos Markdown.

**Limpeza (opcional):**

*   Voc√™ pode adicionar o seguinte c√≥digo ao final da fun√ß√£o `main()` para remover o diret√≥rio tempor√°rio ap√≥s o download, se desejar:

```python
shutil.rmtree(temp_output_directory)
print(f"Temporary output directory removed: {temp_output_directory}")
```

Esta solu√ß√£o alternativa permite que voc√™ baixe os arquivos gerados pelo `crawl4ai` como um arquivo zipado, mesmo que esteja enfrentando problemas com o argumento `output_dir`.

"""

# Instalar depend√™ncias
!pip install -U crawl4ai
!crawl4ai-setup
!crawl4ai-doctor
!python -m playwright install --with-deps chromium

# Importar bibliotecas
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
import os
import shutil
from google.colab import files

# Definir o diret√≥rio tempor√°rio para salvar os arquivos
temp_output_directory = "/content/crawl4ai_temp_output"

# Fun√ß√£o para rastreamento sequencial
async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")

    browser_config = BrowserConfig(
        headless=True,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
        # Sem output_dir
    )

    # Criar o crawler (abre o navegador)
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "session1"  # Reutilizar a mesma sess√£o em todas as URLs
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")

                # Salvar o arquivo temporariamente
                filename = os.path.join(temp_output_directory, f"{url.replace('/', '_')}.md")
                os.makedirs(os.path.dirname(filename), exist_ok=True)
                with open(filename, "w") as f:
                    f.write(result.markdown_v2.raw_markdown)
                print(f"Markdown saved to: {filename}")

            else:
                print(f"Failed: {url} - Error: {result.error_message}")
    finally:
        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)
        await crawler.close()

# Fun√ß√£o principal - ‚ö†Ô∏èINSIRA AQUI AS URLs para Crawler‚ö†Ô∏è
async def main():
    urls = [
        "https://ai.pydantic.dev/agents/",
        "https://ai.pydantic.dev/models/",
        "https://ai.pydantic.dev/dependencies/",
        "https://ai.pydantic.dev/tools/"
    ]
    await crawl_sequential(urls)

    # Compactar a pasta de sa√≠da
    shutil.make_archive("crawl4ai_output", 'zip', temp_output_directory)
    print(f"Output directory zipped to crawl4ai_output.zip")

    # Disponibilizar para download
    files.download("crawl4ai_output.zip")

# Executar a fun√ß√£o principal
await main()

"""# V3.1 - üî• Faz o Crawler e baixa diretamente no Desktop üî• (COM SEPARA√á√ÉO DE C√âLULAS)
OBS.: Eu estava colocando pra salvar no Drive mas a a vers√£o do crawl4ai (0.4.247) que voc√™ est√° usando n√£o suporta o argumento output_dir no construtor CrawlerRunConfig.

**Explica√ß√µes:**

1. **`temp_output_directory = "/content/crawl4ai_temp_output"`:** Define um diret√≥rio tempor√°rio dentro do ambiente do Colab para armazenar os arquivos Markdown.
2. **`crawl_sequential` (modificada):**
    *   O `output_dir` foi removido do `CrawlerRunConfig`.
    *   Os arquivos s√£o salvos temporariamente no `temp_output_directory` usando a l√≥gica que discutimos anteriormente (`os.makedirs`, `with open...`).
3. **`shutil.make_archive("crawl4ai_output", 'zip', temp_output_directory)`:**
    *   Ap√≥s o rastreamento, esta linha usa a fun√ß√£o `make_archive` do m√≥dulo `shutil` para compactar o diret√≥rio tempor√°rio em um arquivo chamado `crawl4ai_output.zip`.
4. **`files.download("crawl4ai_output.zip")`:** Esta linha usa a fun√ß√£o `download` do `google.colab.files` para disponibilizar o arquivo zipado para download no seu navegador.

**Como executar:**

1. Execute o c√≥digo.
2. O c√≥digo ir√° rastrear as URLs e salvar os arquivos Markdown temporariamente.
3. Ao final da execu√ß√£o, o arquivo `crawl4ai_output.zip` ser√° baixado automaticamente pelo seu navegador.
4. Descompacte o arquivo `.zip` para acessar os arquivos Markdown.

üìå**Fluxo de trabalho recomendado:** (üìåNOVIDADE v3.1üìå)

1. **Execute a c√©lula de depend√™ncias uma vez** no in√≠cio ou sempre que precisar instalar ou atualizar uma biblioteca.
2. **Reinicie o ambiente de execu√ß√£o (Runtime -> Restart runtime)** ap√≥s instalar as depend√™ncias (especialmente se voc√™ atualizou uma biblioteca). Isso garante que as novas vers√µes das bibliotecas sejam carregadas corretamente.
3. **Execute as c√©lulas de c√≥digo principal** sempre que fizer altera√ß√µes no c√≥digo e precisar test√°-las. Voc√™ n√£o precisa executar a c√©lula de depend√™ncias novamente, a menos que precise alterar as bibliotecas instaladas.

**Limpeza (opcional):**

*   Voc√™ pode adicionar o seguinte c√≥digo ao final da fun√ß√£o `main()` para remover o diret√≥rio tempor√°rio ap√≥s o download, se desejar:

```python
shutil.rmtree(temp_output_directory)
print(f"Temporary output directory removed: {temp_output_directory}")
```

Esta solu√ß√£o alternativa permite que voc√™ baixe os arquivos gerados pelo `crawl4ai` como um arquivo zipado, mesmo que esteja enfrentando problemas com o argumento `output_dir`.

## 1. C√©lula de depend√™ncias:
"""

# Instalar depend√™ncias (execute esta c√©lula apenas uma vez ou quando precisar atualizar as bibliotecas)
!pip install -U crawl4ai
!crawl4ai-setup
!crawl4ai-doctor
!python -m playwright install --with-deps chromium

"""## 2. C√©lulas de c√≥digo principal:"""

# Importar bibliotecas (execute esta c√©lula sempre que reiniciar o ambiente de execu√ß√£o)
import asyncio
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
import os
import shutil
from google.colab import files

# Definir o diret√≥rio tempor√°rio para salvar os arquivos
temp_output_directory = "/content/crawl4ai_temp_output"

# ... (o restante do seu c√≥digo)

"""## 3. Fluxo de trabalho recomendado:"""

# Fun√ß√£o para rastreamento sequencial
async def crawl_sequential(urls: List[str]):
    print("\n=== Sequential Crawling with Session Reuse ===")

    browser_config = BrowserConfig(
        headless=True,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )

    crawl_config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
        # Sem output_dir
    )

    # Criar o crawler (abre o navegador)
    crawler = AsyncWebCrawler(config=browser_config)
    await crawler.start()

    try:
        session_id = "session1"  # Reutilizar a mesma sess√£o em todas as URLs
        for url in urls:
            result = await crawler.arun(
                url=url,
                config=crawl_config,
                session_id=session_id
            )
            if result.success:
                print(f"Successfully crawled: {url}")
                print(f"Markdown length: {len(result.markdown_v2.raw_markdown)}")

                # Salvar o arquivo temporariamente
                filename = os.path.join(temp_output_directory, f"{url.replace('/', '_')}.md")
                os.makedirs(os.path.dirname(filename), exist_ok=True)
                with open(filename, "w") as f:
                    f.write(result.markdown_v2.raw_markdown)
                print(f"Markdown saved to: {filename}")

            else:
                print(f"Failed: {url} - Error: {result.error_message}")
    finally:
        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)
        await crawler.close()

# Fun√ß√£o principal - ‚ö†Ô∏èINSIRA AQUI AS URLs para Crawler‚ö†Ô∏è
async def main():
    urls = [
        "https://ai.pydantic.dev/agents/",
        "https://ai.pydantic.dev/models/",
        "https://ai.pydantic.dev/dependencies/",
        "https://ai.pydantic.dev/tools/"
    ]
    await crawl_sequential(urls)

    # Compactar a pasta de sa√≠da
    shutil.make_archive("crawl4ai_output", 'zip', temp_output_directory)
    print(f"Output directory zipped to crawl4ai_output.zip")

    # Disponibilizar para download
    files.download("crawl4ai_output.zip")

# Executar a fun√ß√£o principal
await main()