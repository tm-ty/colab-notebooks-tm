{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIEuKB/WqAnKMqq/t7Isi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tm-ty/colab-notebooks-tm/blob/main/%F0%9F%94%A5Crawler_em_Massa_(Crawl4AI)_(Parallel_Crawling_Batch)(working)_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v5 - by Claude\n",
        "**Instruções de Uso:**\n",
        "\n",
        "1. Abra um novo notebook no Google Colab.\n",
        "2. Copie e cole o código fornecido em uma célula do notebook.\n",
        "3. Execute a célula (Ctrl+Enter ou clique no botão de play).\n",
        "4. O código irá:\n",
        "   - Instalar as dependências necessárias\n",
        "   - Buscar automaticamente as URLs do sitemap da Pydantic AI\n",
        "   - Rastrear cada página\n",
        "   - Gerar arquivos Markdown\n",
        "   - Criar um arquivo ZIP com todos os Markdowns\n",
        "5. Ao final da execução, um arquivo `crawl4ai_output.zip` será disponibilizado para download.\n",
        "6. Se solicitado, reinicie a sessão do Colab após a instalação das dependências.\n",
        "\n",
        "**Personalização:**\n",
        "\n",
        "Para usar em outro site, altere a URL do sitemap na linha:\n",
        "```python\n",
        "sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n",
        "```"
      ],
      "metadata": {
        "id": "s5AqtCgbp0pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 - by Claude - CÓDIGO COMPLETO EM UMA ÚNICA CÉLULA"
      ],
      "metadata": {
        "id": "wFIR7S8yr9f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "xDl3-EELpgAF",
        "outputId": "c73ef91c-913c-418a-ad32-87095f18231e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crawl4ai\n",
            "  Downloading Crawl4AI-0.4.247-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting aiosqlite~=0.20 (from crawl4ai)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (5.3.0)\n",
            "Collecting litellm>=1.53.1 (from crawl4ai)\n",
            "  Downloading litellm-1.58.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (1.26.4)\n",
            "Collecting pillow~=10.4 (from crawl4ai)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting playwright>=1.49.0 (from crawl4ai)\n",
            "  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting python-dotenv~=1.0 (from crawl4ai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (4.12.3)\n",
            "Collecting tf-playwright-stealth>=1.1.0 (from crawl4ai)\n",
            "  Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting xxhash~=3.4 (from crawl4ai)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rank-bm25~=0.2 (from crawl4ai)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting aiofiles>=24.1.0 (from crawl4ai)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colorama~=0.4 (from crawl4ai)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (2.10.3)\n",
            "Collecting pyOpenSSL>=24.3.0 (from crawl4ai)\n",
            "  Downloading pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting psutil>=6.1.1 (from crawl4ai)\n",
            "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (3.11.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (8.1.7)\n",
            "Collecting httpx<0.28.0,>=0.23.0 (from litellm>=1.53.1->crawl4ai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.55.3 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (1.57.4)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.53.1->crawl4ai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.10/dist-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright>=1.49.0->crawl4ai)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.10->crawl4ai) (2.27.1)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Collecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pytest-mockito<0.0.5,>=0.0.4 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading pytest-mockito-0.0.4.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.22.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (0.8.2)\n",
            "Requirement already satisfied: pytest>=3 in /usr/local/lib/python3.10/dist-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (8.3.4)\n",
            "Collecting mockito>=1.0.6 (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading mockito-1.5.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.27.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.2.1)\n",
            "Downloading Crawl4AI-0.4.247-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading litellm-1.58.1-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl (33 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mockito-1.5.3-py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: pytest-mockito\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-mockito: filename=pytest_mockito-0.0.4-py3-none-any.whl size=3700 sha256=6f8c7fbf39572386d67f21fa6152ebf1a22927be2f3df9567fb2c677175611a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/72/3d/ad383ec25e3ebb14d12326426166bee101969601f5d35d5462\n",
            "Successfully built pytest-mockito\n",
            "Installing collected packages: xxhash, rank-bm25, python-dotenv, pyee, psutil, pillow, mockito, fake-http-header, colorama, aiosqlite, aiofiles, tiktoken, pytest-mockito, playwright, httpx, tf-playwright-stealth, pyOpenSSL, litellm, crawl4ai\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 24.2.1\n",
            "    Uninstalling pyOpenSSL-24.2.1:\n",
            "      Successfully uninstalled pyOpenSSL-24.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 aiosqlite-0.20.0 colorama-0.4.6 crawl4ai-0.4.247 fake-http-header-0.3.5 httpx-0.27.2 litellm-1.58.1 mockito-1.5.3 pillow-10.4.0 playwright-1.49.1 psutil-6.1.1 pyOpenSSL-25.0.0 pyee-12.0.0 pytest-mockito-0.0.4 python-dotenv-1.0.1 rank-bm25-0.2.2 tf-playwright-stealth-1.1.0 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "psutil"
                ]
              },
              "id": "8c697835b3644f6784543d28c535aa3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m[INIT].... → Running post-installation setup...\u001b[0m\n",
            "\u001b[36m[INIT].... → Installing Playwright browsers...\u001b[0m\n",
            "Installing dependencies...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [61.9 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,200 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,599 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,560 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,527 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,227 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,645 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,663 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,518 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,859 kB]\n",
            "Fetched 28.3 MB in 3s (9,005 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk-bridge2.0-0 set to manually installed.\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatk1.0-0 set to manually installed.\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libatspi2.0-0 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libfontconfig1 set to manually installed.\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 set to manually installed.\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcomposite1 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libcups2 set to manually installed.\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdbus-1-3 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
            "libfreetype6 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
            "libglib2.0-0 set to manually installed.\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnspr4 set to manually installed.\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libnss3 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-client0 set to manually installed.\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-6 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-encodings xfonts-utils xserver-common\n",
            "Recommended packages:\n",
            "  fonts-ipafont-mincho fonts-tlwg-loma xfonts-base\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf fonts-ipafont-gothic fonts-noto-color-emoji fonts-tlwg-loma-otf fonts-unifont\n",
            "  fonts-wqy-zenhei libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-cyrillic xfonts-encodings\n",
            "  xfonts-scalable xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 16 newly installed, 0 to remove and 62 not upgraded.\n",
            "Need to get 29.6 MB of archives.\n",
            "After this operation, 71.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ipafont-gothic all 00303-21ubuntu1 [3,513 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.042-0ubuntu0.22.04.1 [9,944 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-tlwg-loma-otf all 1:0.7.3-1 [107 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-unifont all 1:14.0.01-1 [3,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-wqy-zenhei all 0.9.45-8 [7,472 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 xfonts-cyrillic all 1:1.0.5 [386 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-scalable all 1:1.0.3-1.2ubuntu1 [306 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 29.6 MB in 2s (11.9 MB/s)\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-ipafont-gothic_00303-21ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../01-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../02-fonts-noto-color-emoji_2.042-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.042-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-tlwg-loma-otf.\n",
            "Preparing to unpack .../03-fonts-tlwg-loma-otf_1%3a0.7.3-1_all.deb ...\n",
            "Unpacking fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Selecting previously unselected package fonts-unifont.\n",
            "Preparing to unpack .../04-fonts-unifont_1%3a14.0.01-1_all.deb ...\n",
            "Unpacking fonts-unifont (1:14.0.01-1) ...\n",
            "Selecting previously unselected package fonts-wqy-zenhei.\n",
            "Preparing to unpack .../05-fonts-wqy-zenhei_0.9.45-8_all.deb ...\n",
            "Unpacking fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../06-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../07-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../08-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../09-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../10-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../11-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-cyrillic.\n",
            "Preparing to unpack .../12-xfonts-cyrillic_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-cyrillic (1:1.0.5) ...\n",
            "Selecting previously unselected package xfonts-scalable.\n",
            "Preparing to unpack .../13-xfonts-scalable_1%3a1.0.3-1.2ubuntu1_all.deb ...\n",
            "Unpacking xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../14-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../15-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up fonts-noto-color-emoji (2.042-0ubuntu0.22.04.1) ...\n",
            "Setting up fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up fonts-unifont (1:14.0.01-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-cyrillic (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Downloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 39.4s\u001b[0K\u001b[1G161.3 MiB [] 0% 22.6s\u001b[0K\u001b[1G161.3 MiB [] 0% 14.2s\u001b[0K\u001b[1G161.3 MiB [] 0% 8.6s\u001b[0K\u001b[1G161.3 MiB [] 1% 5.9s\u001b[0K\u001b[1G161.3 MiB [] 1% 5.0s\u001b[0K\u001b[1G161.3 MiB [] 2% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 3% 4.2s\u001b[0K\u001b[1G161.3 MiB [] 3% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 4% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 4% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 5% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 5% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 7% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 8% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 9% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 10% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 11% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 11% 3.2s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 13% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 13% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 14% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 15% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 16% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 17% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 18% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 18% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 19% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 20% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 21% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 22% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 22% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 23% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 24% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 25% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 26% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 27% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 28% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 29% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 30% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 31% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 32% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 34% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 35% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 38% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 39% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 40% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 41% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 44% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 45% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 46% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 47% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 48% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 49% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 50% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 51% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 52% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 53% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 54% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 56% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 57% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 58% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 59% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 60% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 61% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 62% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 63% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 64% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 65% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 66% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 67% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 68% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 69% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 70% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 71% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 72% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 73% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 76% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 4% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 8% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 21% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 62% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 20.5s\u001b[0K\u001b[1G100.9 MiB [] 0% 14.9s\u001b[0K\u001b[1G100.9 MiB [] 0% 8.1s\u001b[0K\u001b[1G100.9 MiB [] 1% 5.2s\u001b[0K\u001b[1G100.9 MiB [] 2% 3.4s\u001b[0K\u001b[1G100.9 MiB [] 3% 2.7s\u001b[0K\u001b[1G100.9 MiB [] 4% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 6% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 7% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 8% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 9% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 11% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 13% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 14% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 15% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 16% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 17% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 18% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 19% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 21% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 23% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 25% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 28% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 30% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 32% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 34% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 36% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 38% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 40% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 42% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 44% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 47% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 49% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 51% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 56% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 67% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 73% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 77% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 82% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 85% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 87% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 89% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 92% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "\u001b[32m[COMPLETE] ● Playwright installation completed successfully.\u001b[0m\n",
            "\u001b[36m[INIT].... → Starting database initialization...\u001b[0m\n",
            "\u001b[36m[COMPLETE] ● Database backup created at: /root/.crawl4ai/crawl4ai.db.backup_20250114_211219\u001b[0m\n",
            "\u001b[36m[INIT].... → Starting database migration...\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Migration completed. 0 records processed.\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Database initialization completed successfully.\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Post-installation setup completed!\u001b[0m\n",
            "\u001b[0m\u001b[36m[INIT].... → Running Crawl4AI health check...\u001b[0m\n",
            "\u001b[36m[INIT].... → Crawl4AI 0.4.247\u001b[0m\n",
            "\u001b[36m[TEST].... ℹ Testing crawling capabilities...\u001b[0m\n",
            "\u001b[36m[EXPORT].. ℹ Exporting PDF and taking screenshot took 1.68s\u001b[0m\n",
            "\u001b[32m[FETCH]... ↓ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 2.88s\u001b[0m\n",
            "\u001b[36m[SCRAPE].. ◆ Processed https://crawl4ai.com... | Time: 66ms\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m2.95s\u001b[0m\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● ✅ Crawling test passed!\u001b[0m\n",
            "\u001b[0mInstalling dependencies...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-freefont-ttf is already the newest version (20120503-10build1).\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "xfonts-scalable is already the newest version (1:1.0.3-1.2ubuntu1).\n",
            "fonts-ipafont-gothic is already the newest version (00303-21ubuntu1).\n",
            "fonts-tlwg-loma-otf is already the newest version (1:0.7.3-1).\n",
            "fonts-unifont is already the newest version (1:14.0.01-1).\n",
            "fonts-wqy-zenhei is already the newest version (0.9.45-8).\n",
            "xfonts-cyrillic is already the newest version (1:1.0.5).\n",
            "fonts-noto-color-emoji is already the newest version (2.042-0ubuntu0.22.04.1).\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n",
            "Encontradas 42 URLs para rastrear\n",
            "\n",
            "=== Rastreamento Sequencial com Reutilização de Sessão ===\n",
            "[INIT].... → Crawl4AI 0.4.247\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/... | Status: True | Time: 1.80s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/... | Time: 156ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/... | Status: True | Total: 2.05s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/\n",
            "Tamanho do Markdown: 13872\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/agents/... | Status: True | Time: 1.59s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/agents/... | Time: 197ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/agents/... | Status: True | Total: 1.85s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/agents/\n",
            "Tamanho do Markdown: 25064\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_agents_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/contributing/... | Status: True | Time: 1.42s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/contributing/... | Time: 204ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/contributing/... | Status: True | Total: 1.66s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/contributing/\n",
            "Tamanho do Markdown: 8017\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_contributing_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/dependencies/... | Status: True | Time: 1.44s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/dependencies/... | Time: 185ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/dependencies/... | Status: True | Total: 1.66s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/dependencies/\n",
            "Tamanho do Markdown: 16232\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_dependencies_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/help/... | Status: True | Time: 1.35s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/help/... | Time: 78ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/help/... | Status: True | Total: 1.48s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/help/\n",
            "Tamanho do Markdown: 5482\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_help_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/install/... | Status: True | Time: 1.79s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/install/... | Time: 194ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/install/... | Status: True | Total: 2.03s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/install/\n",
            "Tamanho do Markdown: 8169\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_install_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/logfire/... | Status: True | Time: 2.69s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/logfire/... | Time: 283ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/logfire/... | Status: True | Total: 3.01s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/logfire/\n",
            "Tamanho do Markdown: 10058\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_logfire_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/message-history/... | Status: True | Time: 2.33s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/message-history/... | Time: 268ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/message-history/... | Status: True | Total: 2.64s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/message-history/\n",
            "Tamanho do Markdown: 16308\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_message-history_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/models/... | Status: True | Time: 2.39s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/models/... | Time: 382ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/models/... | Status: True | Total: 2.84s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/models/\n",
            "Tamanho do Markdown: 34030\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_models_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/multi-agent-applications/... | Status: True | Time: 3.87s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/multi-agent-applications/... | Time: 150ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/multi-agent-applications/... | Status: True | Total: 4.08s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/multi-agent-applications/\n",
            "Tamanho do Markdown: 17868\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_multi-agent-applications_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/results/... | Status: True | Time: 1.46s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/results/... | Time: 179ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/results/... | Status: True | Total: 1.68s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/results/\n",
            "Tamanho do Markdown: 17598\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_results_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/testing-evals/... | Status: True | Time: 1.54s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/testing-evals/... | Time: 315ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/testing-evals/... | Status: True | Total: 1.88s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/testing-evals/\n",
            "Tamanho do Markdown: 25493\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_testing-evals_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/tools/... | Status: True | Time: 4.15s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/tools/... | Time: 456ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/tools/... | Status: True | Total: 4.66s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/tools/\n",
            "Tamanho do Markdown: 19869\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_tools_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/troubleshooting/... | Status: True | Time: 2.38s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/troubleshooting/... | Time: 123ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/troubleshooting/... | Status: True | Total: 2.57s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/troubleshooting/\n",
            "Tamanho do Markdown: 7424\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_troubleshooting_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/agent/... | Status: True | Time: 3.60s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/agent/... | Time: 2452ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/agent/... | Status: True | Total: 6.09s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/agent/\n",
            "Tamanho do Markdown: 164548\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_agent_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/exceptions/... | Status: True | Time: 2.14s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/exceptions/... | Time: 270ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/exceptions/... | Status: True | Total: 2.47s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/exceptions/\n",
            "Tamanho do Markdown: 12779\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_exceptions_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/format_as_xml/... | Status: True | Time: 2.20s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/format_as_xml/... | Time: 178ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/format_as_xml/... | Status: True | Total: 2.43s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/format_as_xml/\n",
            "Tamanho do Markdown: 10449\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_format_as_xml_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/messages/... | Status: True | Time: 2.59s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/messages/... | Time: 766ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/messages/... | Status: True | Total: 3.39s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/messages/\n",
            "Tamanho do Markdown: 74426\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_messages_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/result/... | Status: True | Time: 1.50s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/result/... | Time: 553ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/result/... | Status: True | Total: 2.10s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/result/\n",
            "Tamanho do Markdown: 57961\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_result_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/settings/... | Status: True | Time: 1.30s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/settings/... | Time: 100ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/settings/... | Status: True | Total: 1.45s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/settings/\n",
            "Tamanho do Markdown: 9533\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_settings_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/tools/... | Status: True | Time: 1.50s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/tools/... | Time: 411ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/tools/... | Status: True | Total: 1.96s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/tools/\n",
            "Tamanho do Markdown: 42394\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_tools_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/usage/... | Status: True | Time: 2.05s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/usage/... | Time: 646ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/usage/... | Status: True | Total: 2.75s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/usage/\n",
            "Tamanho do Markdown: 21651\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_usage_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/anthropic/... | Status: True | Time: 2.33s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/anthropic/... | Time: 427ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/anthropic/... | Status: True | Total: 2.82s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/anthropic/\n",
            "Tamanho do Markdown: 21773\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_anthropic_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/base/... | Status: True | Time: 2.32s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/base/... | Time: 448ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/base/... | Status: True | Total: 2.82s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/base/\n",
            "Tamanho do Markdown: 26066\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_base_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/function/... | Status: True | Time: 2.48s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/function/... | Time: 500ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/function/... | Status: True | Total: 3.07s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/function/\n",
            "Tamanho do Markdown: 26582\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_function_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/gemini/... | Status: True | Time: 2.65s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/gemini/... | Time: 549ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/gemini/... | Status: True | Total: 3.26s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/gemini/\n",
            "Tamanho do Markdown: 26571\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_gemini_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/groq/... | Status: True | Time: 1.61s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/groq/... | Time: 409ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/groq/... | Status: True | Total: 2.05s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/groq/\n",
            "Tamanho do Markdown: 21439\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_groq_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/mistral/... | Status: True | Time: 1.38s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/mistral/... | Time: 403ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/mistral/... | Status: True | Total: 1.84s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/mistral/\n",
            "Tamanho do Markdown: 36303\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_mistral_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/ollama/... | Status: True | Time: 1.39s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/ollama/... | Time: 212ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/ollama/... | Status: True | Total: 1.64s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/ollama/\n",
            "Tamanho do Markdown: 17579\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_ollama_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/openai/... | Status: True | Time: 1.50s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/openai/... | Time: 260ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/openai/... | Status: True | Total: 1.80s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/openai/\n",
            "Tamanho do Markdown: 23221\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_openai_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/test/... | Status: True | Time: 1.34s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/test/... | Time: 407ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/test/... | Status: True | Total: 1.78s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/test/\n",
            "Tamanho do Markdown: 21637\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_test_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/vertexai/... | Status: True | Time: 1.56s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/vertexai/... | Time: 395ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/vertexai/... | Status: True | Total: 2.01s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/api/models/vertexai/\n",
            "Tamanho do Markdown: 25573\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_vertexai_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/... | Status: True | Time: 2.15s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/... | Time: 194ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/... | Status: True | Total: 2.39s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/\n",
            "Tamanho do Markdown: 7667\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/bank-support/... | Status: True | Time: 2.23s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/bank-support/... | Time: 210ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/bank-support/... | Status: True | Total: 2.49s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/bank-support/\n",
            "Tamanho do Markdown: 8816\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_bank-support_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/chat-app/... | Status: True | Time: 2.65s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/chat-app/... | Time: 519ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/chat-app/... | Status: True | Total: 3.21s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/chat-app/\n",
            "Tamanho do Markdown: 18603\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_chat-app_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/flight-booking/... | Status: True | Time: 3.80s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/flight-booking/... | Time: 258ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/flight-booking/... | Status: True | Total: 4.10s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/flight-booking/\n",
            "Tamanho do Markdown: 13607\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_flight-booking_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/pydantic-model/... | Status: True | Time: 1.28s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/pydantic-model/... | Time: 145ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/pydantic-model/... | Status: True | Total: 1.47s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/pydantic-model/\n",
            "Tamanho do Markdown: 7538\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_pydantic-model_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/rag/... | Status: True | Time: 1.41s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/rag/... | Time: 202ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/rag/... | Status: True | Total: 1.65s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/rag/\n",
            "Tamanho do Markdown: 14205\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_rag_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/sql-gen/... | Status: True | Time: 1.38s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/sql-gen/... | Time: 298ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/sql-gen/... | Status: True | Total: 1.72s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/sql-gen/\n",
            "Tamanho do Markdown: 11530\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_sql-gen_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/stream-markdown/... | Status: True | Time: 1.66s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/stream-markdown/... | Time: 207ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/stream-markdown/... | Status: True | Total: 1.93s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/stream-markdown/\n",
            "Tamanho do Markdown: 8721\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_stream-markdown_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/stream-whales/... | Status: True | Time: 2.74s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/stream-whales/... | Time: 329ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/stream-whales/... | Status: True | Total: 3.12s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/stream-whales/\n",
            "Tamanho do Markdown: 9086\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_stream-whales_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/weather-agent/... | Status: True | Time: 2.59s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/weather-agent/... | Time: 224ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/weather-agent/... | Status: True | Total: 2.90s\n",
            "Rastreado com sucesso: https://ai.pydantic.dev/examples/weather-agent/\n",
            "Tamanho do Markdown: 12438\n",
            "Markdown salvo em: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_weather-agent_.md\n",
            "Diretório de saída compactado para crawl4ai_output.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9365c75-357f-4a38-a56f-0ef8c7b977cf\", \"crawl4ai_output.zip\", 203449)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Analisar o XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extrair todas as URLs do sitemap\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao buscar o sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Rastreamento Sequencial com Reutilização de Sessão ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Rastreado com sucesso: {url}\")\n",
        "                print(f\"Tamanho do Markdown: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown salvo em: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Falha: {url} - Erro: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\" # <<-- INSIRA SUA URL NESSA LINHA\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Encontradas {len(urls)} URLs para rastrear\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Diretório de saída compactado para crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "    else:\n",
        "        print(\"Nenhuma URL encontrada para rastrear\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 - by Claude - CÓDIGO COMPLETO **COM SEPARAÇÃO** DE CÉLULAS"
      ],
      "metadata": {
        "id": "Z9yozL8VsQLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Célula de dependências:"
      ],
      "metadata": {
        "id": "powbzekrswVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "_toK0HassvjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Células de código principal:"
      ],
      "metadata": {
        "id": "0-954WApsw27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# ... (o restante do seu código)"
      ],
      "metadata": {
        "id": "Dc-dTZ53sv1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fluxo de trabalho recomendado:"
      ],
      "metadata": {
        "id": "1-Nspgiks45c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Analisar o XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extrair todas as URLs do sitemap\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao buscar o sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Rastreamento Sequencial com Reutilização de Sessão ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Rastreado com sucesso: {url}\")\n",
        "                print(f\"Tamanho do Markdown: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown salvo em: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Falha: {url} - Erro: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\" # <<-- INSIRA SUA URL NESSA LINHA\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Encontradas {len(urls)} URLs para rastrear\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Diretório de saída compactado para crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "    else:\n",
        "        print(\"Nenhuma URL encontrada para rastrear\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "QRmNFIJKsmy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v5 by Gemini Exp 1206 - CÓDIGO COMPLETO EM UMA ÚNICA CÉLULA\n",
        "\n",
        "**Como executar:**\n",
        "\n",
        "1. Execute a célula de instalação de dependências **uma vez**. (Se solicitado, reinicie a sessão do Colab após a instalação das dependências.)\n",
        "2. Execute as demais células.\n",
        "3. O código irá:\n",
        "    *   Obter as URLs do `sitemap.xml`.\n",
        "    *   Rastrear cada URL.\n",
        "    *   Salvar os arquivos Markdown temporariamente em `/content/crawl4ai_temp_output`.\n",
        "    *   Criar um arquivo `crawl4ai_output.zip` contendo todos os arquivos Markdown.\n",
        "    *   Iniciar o download do arquivo `crawl4ai_output.zip` no seu navegador.\n",
        "\n",
        "Agora você tem um processo automatizado que obtém as URLs do sitemap, rastreia cada uma delas e fornece os resultados em um arquivo zip para download!\n"
      ],
      "metadata": {
        "id": "n1j2-migp4n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 by Gemini Exp 1206 - CÓDIGO COMPLETO EM UMA ÚNICA CÉLULA"
      ],
      "metadata": {
        "id": "dZWlKyeDtfd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências (execute esta célula apenas uma vez)\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Fetches all URLs from a given sitemap URL.\n",
        "\n",
        "    Args:\n",
        "        sitemap_url: URL of the sitemap.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of URLs found in the sitemap.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parse the XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extract all URLs from the sitemap\n",
        "        # The namespace is usually defined in the root element\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or parsing sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir, vamos salvar os arquivos temporariamente e depois zipá-los\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"  # URL do sitemap\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Found {len(urls)} URLs to crawl\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "        # Limpar o diretório temporário (opcional)\n",
        "        shutil.rmtree(temp_output_directory)\n",
        "        print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "    else:\n",
        "        print(\"No URLs found to crawl\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cRKnyi3vp87b",
        "outputId": "87a1cf72-3561-44d7-f911-dd8fcb3d29c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crawl4ai\n",
            "  Downloading Crawl4AI-0.4.247-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting aiosqlite~=0.20 (from crawl4ai)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (5.3.0)\n",
            "Collecting litellm>=1.53.1 (from crawl4ai)\n",
            "  Downloading litellm-1.58.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (1.26.4)\n",
            "Collecting pillow~=10.4 (from crawl4ai)\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting playwright>=1.49.0 (from crawl4ai)\n",
            "  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting python-dotenv~=1.0 (from crawl4ai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (4.12.3)\n",
            "Collecting tf-playwright-stealth>=1.1.0 (from crawl4ai)\n",
            "  Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting xxhash~=3.4 (from crawl4ai)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rank-bm25~=0.2 (from crawl4ai)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting aiofiles>=24.1.0 (from crawl4ai)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colorama~=0.4 (from crawl4ai)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (2.10.3)\n",
            "Collecting pyOpenSSL>=24.3.0 (from crawl4ai)\n",
            "  Downloading pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting psutil>=6.1.1 (from crawl4ai)\n",
            "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (3.11.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (8.1.7)\n",
            "Collecting httpx<0.28.0,>=0.23.0 (from litellm>=1.53.1->crawl4ai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.55.3 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (1.57.4)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.53.1->crawl4ai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.10/dist-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright>=1.49.0->crawl4ai)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.10->crawl4ai) (2.27.1)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.10/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Collecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pytest-mockito<0.0.5,>=0.0.4 (from tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading pytest-mockito-0.0.4.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.22.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (0.8.2)\n",
            "Requirement already satisfied: pytest>=3 in /usr/local/lib/python3.10/dist-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (8.3.4)\n",
            "Collecting mockito>=1.0.6 (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai)\n",
            "  Downloading mockito-1.5.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (4.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.27.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.23.0->litellm>=1.53.1->crawl4ai) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.2.1)\n",
            "Downloading Crawl4AI-0.4.247-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading litellm-1.58.1-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl (33 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mockito-1.5.3-py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: pytest-mockito\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-mockito: filename=pytest_mockito-0.0.4-py3-none-any.whl size=3700 sha256=9c2fa3bd10cbf84431954ad50057eba4af7ef07e541c0277c5227ac73c768356\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/72/3d/ad383ec25e3ebb14d12326426166bee101969601f5d35d5462\n",
            "Successfully built pytest-mockito\n",
            "Installing collected packages: xxhash, rank-bm25, python-dotenv, pyee, psutil, pillow, mockito, fake-http-header, colorama, aiosqlite, aiofiles, tiktoken, pytest-mockito, playwright, httpx, tf-playwright-stealth, pyOpenSSL, litellm, crawl4ai\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 24.2.1\n",
            "    Uninstalling pyOpenSSL-24.2.1:\n",
            "      Successfully uninstalled pyOpenSSL-24.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 aiosqlite-0.20.0 colorama-0.4.6 crawl4ai-0.4.247 fake-http-header-0.3.5 httpx-0.27.2 litellm-1.58.1 mockito-1.5.3 pillow-10.4.0 playwright-1.49.1 psutil-6.1.1 pyOpenSSL-25.0.0 pyee-12.0.0 pytest-mockito-0.0.4 python-dotenv-1.0.1 rank-bm25-0.2.2 tf-playwright-stealth-1.1.0 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "psutil"
                ]
              },
              "id": "f6f07fec0d704deeb1d6342e75b3172f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m[INIT].... → Running post-installation setup...\u001b[0m\n",
            "\u001b[36m[INIT].... → Installing Playwright browsers...\u001b[0m\n",
            "Installing dependencies...\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [45.2 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,560 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,527 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,227 kB]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [61.9 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,200 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,859 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,599 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,518 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,663 kB]\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,645 kB]\n",
            "Fetched 28.3 MB in 4s (7,968 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk-bridge2.0-0 set to manually installed.\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatk1.0-0 set to manually installed.\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libatspi2.0-0 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libfontconfig1 set to manually installed.\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 set to manually installed.\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcomposite1 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libcups2 set to manually installed.\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdbus-1-3 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
            "libfreetype6 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
            "libglib2.0-0 set to manually installed.\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnspr4 set to manually installed.\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libnss3 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-client0 set to manually installed.\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-6 set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-encodings xfonts-utils xserver-common\n",
            "Recommended packages:\n",
            "  fonts-ipafont-mincho fonts-tlwg-loma xfonts-base\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf fonts-ipafont-gothic fonts-noto-color-emoji fonts-tlwg-loma-otf fonts-unifont\n",
            "  fonts-wqy-zenhei libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-cyrillic xfonts-encodings\n",
            "  xfonts-scalable xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 16 newly installed, 0 to remove and 64 not upgraded.\n",
            "Need to get 29.6 MB of archives.\n",
            "After this operation, 71.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ipafont-gothic all 00303-21ubuntu1 [3,513 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.042-0ubuntu0.22.04.1 [9,944 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-tlwg-loma-otf all 1:0.7.3-1 [107 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-unifont all 1:14.0.01-1 [3,551 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-wqy-zenhei all 0.9.45-8 [7,472 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 xfonts-cyrillic all 1:1.0.5 [386 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-scalable all 1:1.0.3-1.2ubuntu1 [306 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 29.6 MB in 2s (19.0 MB/s)\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-ipafont-gothic_00303-21ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../01-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../02-fonts-noto-color-emoji_2.042-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.042-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-tlwg-loma-otf.\n",
            "Preparing to unpack .../03-fonts-tlwg-loma-otf_1%3a0.7.3-1_all.deb ...\n",
            "Unpacking fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Selecting previously unselected package fonts-unifont.\n",
            "Preparing to unpack .../04-fonts-unifont_1%3a14.0.01-1_all.deb ...\n",
            "Unpacking fonts-unifont (1:14.0.01-1) ...\n",
            "Selecting previously unselected package fonts-wqy-zenhei.\n",
            "Preparing to unpack .../05-fonts-wqy-zenhei_0.9.45-8_all.deb ...\n",
            "Unpacking fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../06-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../07-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../08-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../09-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../10-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../11-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-cyrillic.\n",
            "Preparing to unpack .../12-xfonts-cyrillic_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-cyrillic (1:1.0.5) ...\n",
            "Selecting previously unselected package xfonts-scalable.\n",
            "Preparing to unpack .../13-xfonts-scalable_1%3a1.0.3-1.2ubuntu1_all.deb ...\n",
            "Unpacking xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../14-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../15-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up fonts-noto-color-emoji (2.042-0ubuntu0.22.04.1) ...\n",
            "Setting up fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up fonts-unifont (1:14.0.01-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-cyrillic (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Downloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 0% 43.6s\u001b[0K\u001b[1G161.3 MiB [] 0% 26.9s\u001b[0K\u001b[1G161.3 MiB [] 0% 21.8s\u001b[0K\u001b[1G161.3 MiB [] 0% 10.1s\u001b[0K\u001b[1G161.3 MiB [] 1% 5.4s\u001b[0K\u001b[1G161.3 MiB [] 2% 4.8s\u001b[0K\u001b[1G161.3 MiB [] 2% 5.4s\u001b[0K\u001b[1G161.3 MiB [] 2% 4.7s\u001b[0K\u001b[1G161.3 MiB [] 3% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 4% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 4% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 5% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 7% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 8% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 9% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 9% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 10% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 10% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 10% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 11% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.5s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 12% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 13% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 13% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 13% 4.2s\u001b[0K\u001b[1G161.3 MiB [] 13% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 13% 4.4s\u001b[0K\u001b[1G161.3 MiB [] 14% 4.4s\u001b[0K\u001b[1G161.3 MiB [] 14% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 15% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 16% 4.4s\u001b[0K\u001b[1G161.3 MiB [] 16% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 16% 4.6s\u001b[0K\u001b[1G161.3 MiB [] 16% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 16% 4.6s\u001b[0K\u001b[1G161.3 MiB [] 17% 4.6s\u001b[0K\u001b[1G161.3 MiB [] 17% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 18% 4.5s\u001b[0K\u001b[1G161.3 MiB [] 18% 4.3s\u001b[0K\u001b[1G161.3 MiB [] 19% 4.2s\u001b[0K\u001b[1G161.3 MiB [] 20% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 21% 4.0s\u001b[0K\u001b[1G161.3 MiB [] 21% 3.9s\u001b[0K\u001b[1G161.3 MiB [] 22% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 23% 3.7s\u001b[0K\u001b[1G161.3 MiB [] 23% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 24% 3.6s\u001b[0K\u001b[1G161.3 MiB [] 25% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 26% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 26% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 27% 3.3s\u001b[0K\u001b[1G161.3 MiB [] 27% 3.2s\u001b[0K\u001b[1G161.3 MiB [] 28% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 29% 3.2s\u001b[0K\u001b[1G161.3 MiB [] 30% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 31% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 32% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 33% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 33% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 34% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 35% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 36% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 37% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 38% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 39% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 40% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 41% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 42% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 43% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 44% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 45% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 46% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 46% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 47% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 48% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 48% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 49% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 50% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 51% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 52% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 53% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 54% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 54% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 55% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 56% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 57% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 58% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 59% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 60% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 61% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 62% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 63% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 64% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 65% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 66% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 67% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 68% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 69% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 70% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 70% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 71% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 72% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 74% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 75% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 76% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 77% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 78% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 79% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 80% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 82% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 83% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 93% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 44% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 30.9s\u001b[0K\u001b[1G100.9 MiB [] 0% 30.2s\u001b[0K\u001b[1G100.9 MiB [] 0% 38.8s\u001b[0K\u001b[1G100.9 MiB [] 0% 36.8s\u001b[0K\u001b[1G100.9 MiB [] 0% 37.6s\u001b[0K\u001b[1G100.9 MiB [] 0% 26.2s\u001b[0K\u001b[1G100.9 MiB [] 0% 18.8s\u001b[0K\u001b[1G100.9 MiB [] 0% 21.7s\u001b[0K\u001b[1G100.9 MiB [] 1% 27.1s\u001b[0K\u001b[1G100.9 MiB [] 1% 28.8s\u001b[0K\u001b[1G100.9 MiB [] 1% 29.7s\u001b[0K\u001b[1G100.9 MiB [] 1% 29.4s\u001b[0K\u001b[1G100.9 MiB [] 1% 27.8s\u001b[0K\u001b[1G100.9 MiB [] 1% 27.9s\u001b[0K\u001b[1G100.9 MiB [] 1% 22.2s\u001b[0K\u001b[1G100.9 MiB [] 2% 15.9s\u001b[0K\u001b[1G100.9 MiB [] 3% 10.6s\u001b[0K\u001b[1G100.9 MiB [] 4% 9.9s\u001b[0K\u001b[1G100.9 MiB [] 4% 9.3s\u001b[0K\u001b[1G100.9 MiB [] 5% 9.6s\u001b[0K\u001b[1G100.9 MiB [] 5% 9.7s\u001b[0K\u001b[1G100.9 MiB [] 5% 9.8s\u001b[0K\u001b[1G100.9 MiB [] 5% 9.2s\u001b[0K\u001b[1G100.9 MiB [] 7% 7.7s\u001b[0K\u001b[1G100.9 MiB [] 7% 6.9s\u001b[0K\u001b[1G100.9 MiB [] 8% 6.4s\u001b[0K\u001b[1G100.9 MiB [] 9% 5.9s\u001b[0K\u001b[1G100.9 MiB [] 10% 5.4s\u001b[0K\u001b[1G100.9 MiB [] 11% 5.3s\u001b[0K\u001b[1G100.9 MiB [] 12% 5.0s\u001b[0K\u001b[1G100.9 MiB [] 13% 4.5s\u001b[0K\u001b[1G100.9 MiB [] 14% 4.2s\u001b[0K\u001b[1G100.9 MiB [] 15% 4.2s\u001b[0K\u001b[1G100.9 MiB [] 15% 4.1s\u001b[0K\u001b[1G100.9 MiB [] 16% 3.9s\u001b[0K\u001b[1G100.9 MiB [] 18% 3.6s\u001b[0K\u001b[1G100.9 MiB [] 19% 3.4s\u001b[0K\u001b[1G100.9 MiB [] 20% 3.2s\u001b[0K\u001b[1G100.9 MiB [] 21% 3.1s\u001b[0K\u001b[1G100.9 MiB [] 22% 2.9s\u001b[0K\u001b[1G100.9 MiB [] 23% 2.9s\u001b[0K\u001b[1G100.9 MiB [] 24% 2.8s\u001b[0K\u001b[1G100.9 MiB [] 25% 2.7s\u001b[0K\u001b[1G100.9 MiB [] 26% 2.6s\u001b[0K\u001b[1G100.9 MiB [] 27% 2.5s\u001b[0K\u001b[1G100.9 MiB [] 28% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 29% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 30% 2.4s\u001b[0K\u001b[1G100.9 MiB [] 30% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 31% 2.3s\u001b[0K\u001b[1G100.9 MiB [] 32% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 33% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 33% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 34% 2.1s\u001b[0K\u001b[1G100.9 MiB [] 35% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 37% 2.0s\u001b[0K\u001b[1G100.9 MiB [] 38% 1.9s\u001b[0K\u001b[1G100.9 MiB [] 39% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 40% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 41% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 42% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 43% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 44% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 46% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 48% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 50% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 52% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 54% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 55% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 56% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 58% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 60% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 61% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 63% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 66% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 68% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 70% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 71% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 73% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 76% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 77% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 78% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 81% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 83% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 85% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 89% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 92% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 95% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 96% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 97% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 99% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "\u001b[32m[COMPLETE] ● Playwright installation completed successfully.\u001b[0m\n",
            "\u001b[36m[INIT].... → Starting database initialization...\u001b[0m\n",
            "\u001b[36m[COMPLETE] ● Database backup created at: /root/.crawl4ai/crawl4ai.db.backup_20250114_213157\u001b[0m\n",
            "\u001b[36m[INIT].... → Starting database migration...\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Migration completed. 0 records processed.\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Database initialization completed successfully.\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● Post-installation setup completed!\u001b[0m\n",
            "\u001b[0m\u001b[36m[INIT].... → Running Crawl4AI health check...\u001b[0m\n",
            "\u001b[36m[INIT].... → Crawl4AI 0.4.247\u001b[0m\n",
            "\u001b[36m[TEST].... ℹ Testing crawling capabilities...\u001b[0m\n",
            "\u001b[36m[EXPORT].. ℹ Exporting PDF and taking screenshot took 1.60s\u001b[0m\n",
            "\u001b[32m[FETCH]... ↓ https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Time: 4.40s\u001b[0m\n",
            "\u001b[36m[SCRAPE].. ◆ Processed https://crawl4ai.com... | Time: 65ms\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● https://crawl4ai.com... | Status: \u001b[32mTrue\u001b[0m | Total: \u001b[33m4.47s\u001b[0m\u001b[0m\n",
            "\u001b[32m[COMPLETE] ● ✅ Crawling test passed!\u001b[0m\n",
            "\u001b[0mInstalling dependencies...\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-freefont-ttf is already the newest version (20120503-10build1).\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "xfonts-scalable is already the newest version (1:1.0.3-1.2ubuntu1).\n",
            "fonts-ipafont-gothic is already the newest version (00303-21ubuntu1).\n",
            "fonts-tlwg-loma-otf is already the newest version (1:0.7.3-1).\n",
            "fonts-unifont is already the newest version (1:14.0.01-1).\n",
            "fonts-wqy-zenhei is already the newest version (0.9.45-8).\n",
            "xfonts-cyrillic is already the newest version (1:1.0.5).\n",
            "fonts-noto-color-emoji is already the newest version (2.042-0ubuntu0.22.04.1).\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.2).\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 64 not upgraded.\n",
            "Found 42 URLs to crawl\n",
            "\n",
            "=== Sequential Crawling with Session Reuse ===\n",
            "[INIT].... → Crawl4AI 0.4.247\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/... | Status: True | Time: 2.63s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/... | Time: 312ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/... | Status: True | Total: 3.03s\n",
            "Successfully crawled: https://ai.pydantic.dev/\n",
            "Markdown length: 13872\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/agents/... | Status: True | Time: 2.74s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/agents/... | Time: 509ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/agents/... | Status: True | Total: 3.30s\n",
            "Successfully crawled: https://ai.pydantic.dev/agents/\n",
            "Markdown length: 25064\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_agents_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/contributing/... | Status: True | Time: 2.28s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/contributing/... | Time: 151ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/contributing/... | Status: True | Total: 2.48s\n",
            "Successfully crawled: https://ai.pydantic.dev/contributing/\n",
            "Markdown length: 8017\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_contributing_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/dependencies/... | Status: True | Time: 2.83s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/dependencies/... | Time: 294ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/dependencies/... | Status: True | Total: 3.18s\n",
            "Successfully crawled: https://ai.pydantic.dev/dependencies/\n",
            "Markdown length: 16235\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_dependencies_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/help/... | Status: True | Time: 2.13s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/help/... | Time: 153ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/help/... | Status: True | Total: 2.33s\n",
            "Successfully crawled: https://ai.pydantic.dev/help/\n",
            "Markdown length: 5482\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_help_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/install/... | Status: True | Time: 2.13s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/install/... | Time: 194ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/install/... | Status: True | Total: 2.36s\n",
            "Successfully crawled: https://ai.pydantic.dev/install/\n",
            "Markdown length: 8169\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_install_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/logfire/... | Status: True | Time: 2.48s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/logfire/... | Time: 146ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/logfire/... | Status: True | Total: 2.66s\n",
            "Successfully crawled: https://ai.pydantic.dev/logfire/\n",
            "Markdown length: 10058\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_logfire_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/message-history/... | Status: True | Time: 1.39s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/message-history/... | Time: 100ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/message-history/... | Status: True | Total: 1.54s\n",
            "Successfully crawled: https://ai.pydantic.dev/message-history/\n",
            "Markdown length: 16308\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_message-history_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/models/... | Status: True | Time: 1.53s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/models/... | Time: 302ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/models/... | Status: True | Total: 1.88s\n",
            "Successfully crawled: https://ai.pydantic.dev/models/\n",
            "Markdown length: 34030\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_models_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/multi-agent-applications/... | Status: True | Time: 2.50s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/multi-agent-applications/... | Time: 158ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/multi-agent-applications/... | Status: True | Total: 2.70s\n",
            "Successfully crawled: https://ai.pydantic.dev/multi-agent-applications/\n",
            "Markdown length: 17868\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_multi-agent-applications_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/results/... | Status: True | Time: 1.45s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/results/... | Time: 218ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/results/... | Status: True | Total: 1.70s\n",
            "Successfully crawled: https://ai.pydantic.dev/results/\n",
            "Markdown length: 17598\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_results_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/testing-evals/... | Status: True | Time: 1.60s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/testing-evals/... | Time: 438ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/testing-evals/... | Status: True | Total: 2.09s\n",
            "Successfully crawled: https://ai.pydantic.dev/testing-evals/\n",
            "Markdown length: 25493\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_testing-evals_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/tools/... | Status: True | Time: 4.12s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/tools/... | Time: 454ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/tools/... | Status: True | Total: 4.62s\n",
            "Successfully crawled: https://ai.pydantic.dev/tools/\n",
            "Markdown length: 19866\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_tools_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/troubleshooting/... | Status: True | Time: 2.08s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/troubleshooting/... | Time: 147ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/troubleshooting/... | Status: True | Total: 2.29s\n",
            "Successfully crawled: https://ai.pydantic.dev/troubleshooting/\n",
            "Markdown length: 7424\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_troubleshooting_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/agent/... | Status: True | Time: 3.66s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/agent/... | Time: 2721ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/agent/... | Status: True | Total: 6.44s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/agent/\n",
            "Markdown length: 164548\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_agent_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/exceptions/... | Status: True | Time: 1.64s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/exceptions/... | Time: 176ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/exceptions/... | Status: True | Total: 1.85s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/exceptions/\n",
            "Markdown length: 12779\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_exceptions_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/format_as_xml/... | Status: True | Time: 1.31s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/format_as_xml/... | Time: 116ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/format_as_xml/... | Status: True | Total: 1.48s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/format_as_xml/\n",
            "Markdown length: 10449\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_format_as_xml_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/messages/... | Status: True | Time: 2.66s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/messages/... | Time: 775ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/messages/... | Status: True | Total: 3.48s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/messages/\n",
            "Markdown length: 74426\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_messages_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/result/... | Status: True | Time: 1.64s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/result/... | Time: 504ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/result/... | Status: True | Total: 2.21s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/result/\n",
            "Markdown length: 57961\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_result_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/settings/... | Status: True | Time: 1.31s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/settings/... | Time: 141ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/settings/... | Status: True | Total: 1.49s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/settings/\n",
            "Markdown length: 9533\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_settings_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/tools/... | Status: True | Time: 2.44s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/tools/... | Time: 747ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/tools/... | Status: True | Total: 3.23s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/tools/\n",
            "Markdown length: 42394\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_tools_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/usage/... | Status: True | Time: 2.32s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/usage/... | Time: 636ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/usage/... | Status: True | Total: 3.00s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/usage/\n",
            "Markdown length: 21651\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_usage_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/anthropic/... | Status: True | Time: 2.18s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/anthropic/... | Time: 398ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/anthropic/... | Status: True | Total: 2.62s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/anthropic/\n",
            "Markdown length: 21773\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_anthropic_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/base/... | Status: True | Time: 2.37s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/base/... | Time: 486ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/base/... | Status: True | Total: 2.88s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/base/\n",
            "Markdown length: 26066\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_base_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/function/... | Status: True | Time: 1.78s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/function/... | Time: 226ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/function/... | Status: True | Total: 2.05s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/function/\n",
            "Markdown length: 26582\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_function_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/gemini/... | Status: True | Time: 1.40s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/gemini/... | Time: 320ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/gemini/... | Status: True | Total: 1.77s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/gemini/\n",
            "Markdown length: 26571\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_gemini_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/groq/... | Status: True | Time: 1.35s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/groq/... | Time: 405ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/groq/... | Status: True | Total: 1.83s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/groq/\n",
            "Markdown length: 21439\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_groq_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/mistral/... | Status: True | Time: 1.66s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/mistral/... | Time: 683ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/mistral/... | Status: True | Total: 2.40s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/mistral/\n",
            "Markdown length: 36303\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_mistral_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/ollama/... | Status: True | Time: 2.45s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/ollama/... | Time: 388ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/ollama/... | Status: True | Total: 2.89s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/ollama/\n",
            "Markdown length: 17579\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_ollama_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/openai/... | Status: True | Time: 2.52s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/openai/... | Time: 627ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/openai/... | Status: True | Total: 3.22s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/openai/\n",
            "Markdown length: 23221\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_openai_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/test/... | Status: True | Time: 2.65s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/test/... | Time: 598ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/test/... | Status: True | Total: 3.29s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/test/\n",
            "Markdown length: 21637\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_test_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/api/models/vertexai/... | Status: True | Time: 2.35s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/api/models/vertexai/... | Time: 508ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/api/models/vertexai/... | Status: True | Total: 2.90s\n",
            "Successfully crawled: https://ai.pydantic.dev/api/models/vertexai/\n",
            "Markdown length: 25573\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_api_models_vertexai_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/... | Status: True | Time: 2.12s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/... | Time: 169ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/... | Status: True | Total: 2.35s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/\n",
            "Markdown length: 7667\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/bank-support/... | Status: True | Time: 2.02s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/bank-support/... | Time: 202ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/bank-support/... | Status: True | Total: 2.28s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/bank-support/\n",
            "Markdown length: 8816\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_bank-support_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/chat-app/... | Status: True | Time: 1.82s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/chat-app/... | Time: 297ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/chat-app/... | Status: True | Total: 2.17s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/chat-app/\n",
            "Markdown length: 18603\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_chat-app_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/flight-booking/... | Status: True | Time: 1.99s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/flight-booking/... | Time: 144ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/flight-booking/... | Status: True | Total: 2.18s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/flight-booking/\n",
            "Markdown length: 14155\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_flight-booking_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/pydantic-model/... | Status: True | Time: 1.47s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/pydantic-model/... | Time: 111ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/pydantic-model/... | Status: True | Total: 1.63s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/pydantic-model/\n",
            "Markdown length: 7538\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_pydantic-model_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/rag/... | Status: True | Time: 1.43s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/rag/... | Time: 188ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/rag/... | Status: True | Total: 1.65s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/rag/\n",
            "Markdown length: 14205\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_rag_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/sql-gen/... | Status: True | Time: 1.39s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/sql-gen/... | Time: 277ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/sql-gen/... | Status: True | Total: 1.73s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/sql-gen/\n",
            "Markdown length: 11530\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_sql-gen_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/stream-markdown/... | Status: True | Time: 1.50s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/stream-markdown/... | Time: 224ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/stream-markdown/... | Status: True | Total: 1.75s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/stream-markdown/\n",
            "Markdown length: 8721\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_stream-markdown_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/stream-whales/... | Status: True | Time: 3.03s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/stream-whales/... | Time: 253ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/stream-whales/... | Status: True | Total: 3.34s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/stream-whales/\n",
            "Markdown length: 9086\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_stream-whales_.md\n",
            "[FETCH]... ↓ https://ai.pydantic.dev/examples/weather-agent/... | Status: True | Time: 2.42s\n",
            "[SCRAPE].. ◆ Processed https://ai.pydantic.dev/examples/weather-agent/... | Time: 260ms\n",
            "[COMPLETE] ● https://ai.pydantic.dev/examples/weather-agent/... | Status: True | Total: 2.74s\n",
            "Successfully crawled: https://ai.pydantic.dev/examples/weather-agent/\n",
            "Markdown length: 12438\n",
            "Markdown saved to: /content/crawl4ai_temp_output/https___ai.pydantic.dev_examples_weather-agent_.md\n",
            "Output directory zipped to crawl4ai_output.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_20f748dd-7abe-4374-b9bc-3f45ccaee321\", \"crawl4ai_output.zip\", 203602)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temporary output directory removed: /content/crawl4ai_temp_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 by Gemini Exp 1206 - CÓDIGO COMPLETO **COM SEPARAÇÃO** DE CÉLULAS"
      ],
      "metadata": {
        "id": "fbcq9RPWtmct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Célula de dependências:"
      ],
      "metadata": {
        "id": "N98XJfJfwj5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências (execute esta célula apenas uma vez)\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "ncrjmLNftqos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Células de código principal:"
      ],
      "metadata": {
        "id": "95DQ0k6ywLaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# ... (o restante do seu código)"
      ],
      "metadata": {
        "id": "leSVa1vkwM2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fluxo de trabalho recomendado:"
      ],
      "metadata": {
        "id": "w57adffTwLNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Fetches all URLs from a given sitemap URL.\n",
        "\n",
        "    Args:\n",
        "        sitemap_url: URL of the sitemap.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of URLs found in the sitemap.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parse the XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extract all URLs from the sitemap\n",
        "        # The namespace is usually defined in the root element\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or parsing sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir, vamos salvar os arquivos temporariamente e depois zipá-los\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"  # URL do sitemap\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Found {len(urls)} URLs to crawl\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "        # Limpar o diretório temporário (opcional)\n",
        "        shutil.rmtree(temp_output_directory)\n",
        "        print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "    else:\n",
        "        print(\"No URLs found to crawl\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "WYmxIrQMwNG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}