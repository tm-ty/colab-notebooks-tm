{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIEuKB/WqAnKMqq/t7Isi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tm-ty/colab-notebooks-tm/blob/main/%F0%9F%94%A5Crawler_em_Massa_(Crawl4AI)_(Parallel_Crawling_Batch)(working)_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v5 - by Claude\n",
        "**Instruções de Uso:**\n",
        "\n",
        "1. Abra um novo notebook no Google Colab.\n",
        "2. Copie e cole o código fornecido em uma célula do notebook.\n",
        "3. Execute a célula (Ctrl+Enter ou clique no botão de play).\n",
        "4. O código irá:\n",
        "   - Instalar as dependências necessárias\n",
        "   - Buscar automaticamente as URLs do sitemap da Pydantic AI\n",
        "   - Rastrear cada página\n",
        "   - Gerar arquivos Markdown\n",
        "   - Criar um arquivo ZIP com todos os Markdowns\n",
        "5. Ao final da execução, um arquivo `crawl4ai_output.zip` será disponibilizado para download.\n",
        "6. Se solicitado, reinicie a sessão do Colab após a instalação das dependências.\n",
        "\n",
        "**Personalização:**\n",
        "\n",
        "Para usar em outro site, altere a URL do sitemap na linha:\n",
        "```python\n",
        "sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"\n",
        "```"
      ],
      "metadata": {
        "id": "s5AqtCgbp0pM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 - by Claude - CÓDIGO COMPLETO EM UMA ÚNICA CÉLULA"
      ],
      "metadata": {
        "id": "wFIR7S8yr9f8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xDl3-EELpgAF"
      },
      "outputs": [],
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Analisar o XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extrair todas as URLs do sitemap\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao buscar o sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Rastreamento Sequencial com Reutilização de Sessão ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Rastreado com sucesso: {url}\")\n",
        "                print(f\"Tamanho do Markdown: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown salvo em: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Falha: {url} - Erro: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\" # <<-- INSIRA SUA URL NESSA LINHA\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Encontradas {len(urls)} URLs para rastrear\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Diretório de saída compactado para crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "    else:\n",
        "        print(\"Nenhuma URL encontrada para rastrear\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 - by Claude - CÓDIGO COMPLETO **COM SEPARAÇÃO** DE CÉLULAS"
      ],
      "metadata": {
        "id": "Z9yozL8VsQLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Célula de dependências:"
      ],
      "metadata": {
        "id": "powbzekrswVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "_toK0HassvjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Células de código principal:"
      ],
      "metadata": {
        "id": "0-954WApsw27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# ... (o restante do seu código)"
      ],
      "metadata": {
        "id": "Dc-dTZ53sv1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fluxo de trabalho recomendado:"
      ],
      "metadata": {
        "id": "1-Nspgiks45c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Analisar o XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extrair todas as URLs do sitemap\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao buscar o sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Rastreamento Sequencial com Reutilização de Sessão ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Rastreado com sucesso: {url}\")\n",
        "                print(f\"Tamanho do Markdown: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown salvo em: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Falha: {url} - Erro: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\" # <<-- INSIRA SUA URL NESSA LINHA\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Encontradas {len(urls)} URLs para rastrear\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Diretório de saída compactado para crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "    else:\n",
        "        print(\"Nenhuma URL encontrada para rastrear\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "QRmNFIJKsmy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v5 by Gemini Exp 1206 - CÓDIGO COMPLETO EM UMA ÚNICA CÉLULA\n",
        "\n",
        "**Como executar:**\n",
        "\n",
        "1. Execute a célula de instalação de dependências **uma vez**. (Se solicitado, reinicie a sessão do Colab após a instalação das dependências.)\n",
        "2. Execute as demais células.\n",
        "3. O código irá:\n",
        "    *   Obter as URLs do `sitemap.xml`.\n",
        "    *   Rastrear cada URL.\n",
        "    *   Salvar os arquivos Markdown temporariamente em `/content/crawl4ai_temp_output`.\n",
        "    *   Criar um arquivo `crawl4ai_output.zip` contendo todos os arquivos Markdown.\n",
        "    *   Iniciar o download do arquivo `crawl4ai_output.zip` no seu navegador.\n",
        "\n",
        "Agora você tem um processo automatizado que obtém as URLs do sitemap, rastreia cada uma delas e fornece os resultados em um arquivo zip para download!\n"
      ],
      "metadata": {
        "id": "n1j2-migp4n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 by Gemini Exp 1206 - CÓDIGO COMPLETO EM UMA ÚNICA CÉLULA"
      ],
      "metadata": {
        "id": "dZWlKyeDtfd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências (execute esta célula apenas uma vez)\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Fetches all URLs from a given sitemap URL.\n",
        "\n",
        "    Args:\n",
        "        sitemap_url: URL of the sitemap.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of URLs found in the sitemap.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parse the XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extract all URLs from the sitemap\n",
        "        # The namespace is usually defined in the root element\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or parsing sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir, vamos salvar os arquivos temporariamente e depois zipá-los\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"  # URL do sitemap\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Found {len(urls)} URLs to crawl\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "        # Limpar o diretório temporário (opcional)\n",
        "        shutil.rmtree(temp_output_directory)\n",
        "        print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "    else:\n",
        "        print(\"No URLs found to crawl\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "cRKnyi3vp87b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## v5 by Gemini Exp 1206 - CÓDIGO COMPLETO **COM SEPARAÇÃO** DE CÉLULAS"
      ],
      "metadata": {
        "id": "fbcq9RPWtmct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Célula de dependências:"
      ],
      "metadata": {
        "id": "N98XJfJfwj5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências (execute esta célula apenas uma vez)\n",
        "!pip install -U crawl4ai requests\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "ncrjmLNftqos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Células de código principal:"
      ],
      "metadata": {
        "id": "95DQ0k6ywLaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# ... (o restante do seu código)"
      ],
      "metadata": {
        "id": "leSVa1vkwM2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fluxo de trabalho recomendado:"
      ],
      "metadata": {
        "id": "w57adffTwLNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import requests\n",
        "from xml.etree import ElementTree\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para obter URLs do sitemap\n",
        "def get_urls_from_sitemap(sitemap_url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Fetches all URLs from a given sitemap URL.\n",
        "\n",
        "    Args:\n",
        "        sitemap_url: URL of the sitemap.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: List of URLs found in the sitemap.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Parse the XML\n",
        "        root = ElementTree.fromstring(response.content)\n",
        "\n",
        "        # Extract all URLs from the sitemap\n",
        "        # The namespace is usually defined in the root element\n",
        "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
        "        urls = [loc.text for loc in root.findall('.//ns:loc', namespace)]\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or parsing sitemap: {e}\")\n",
        "        return []\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir, vamos salvar os arquivos temporariamente e depois zipá-los\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_').replace(':', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    sitemap_url = \"https://ai.pydantic.dev/sitemap.xml\"  # URL do sitemap\n",
        "    urls = get_urls_from_sitemap(sitemap_url)\n",
        "\n",
        "    if urls:\n",
        "        print(f\"Found {len(urls)} URLs to crawl\")\n",
        "        await crawl_sequential(urls)\n",
        "\n",
        "        # Compactar a pasta de saída\n",
        "        shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "        print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "        # Disponibilizar para download\n",
        "        files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "        # Limpar o diretório temporário (opcional)\n",
        "        shutil.rmtree(temp_output_directory)\n",
        "        print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "    else:\n",
        "        print(\"No URLs found to crawl\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "WYmxIrQMwNG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}