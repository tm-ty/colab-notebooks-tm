{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY3a+4oShMriRLqXzoxlOR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tm-ty/colab-notebooks-tm/blob/main/Crawler_Manual_(Crawl4AI)_v3_1_Working_(helper_Gemini_exp_1206).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1"
      ],
      "metadata": {
        "id": "NjXU3bXpLlTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## passo 1"
      ],
      "metadata": {
        "id": "AlGVmezfLqS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDDMioOuLb-O"
      },
      "outputs": [],
      "source": [
        "!pip install -U crawl4ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-setup"
      ],
      "metadata": {
        "id": "Y0BnBvpuLenM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-doctor"
      ],
      "metadata": {
        "id": "CtZP0dcJLf5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "gYQDslcWLffA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 2: Adaptar o c√≥digo"
      ],
      "metadata": {
        "id": "oR-iAi_ZLwh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OBS.: As duas linhas baixo n√£o s√£o necess√°rias no Colab\n",
        "# parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "# sys.path.append(parent_dir)"
      ],
      "metadata": {
        "id": "6oDFYo8eLfwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Implantar a solu√ß√£o"
      ],
      "metadata": {
        "id": "2v-0qeFeMt3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In_7xZrwMvlY",
        "outputId": "43e19c1e-386b-40d6-a3d3-6b38e32b0773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 4: Monitorar e obter resultados"
      ],
      "metadata": {
        "id": "vLpHPaQ_NNT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar depend√™ncias\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# Fun√ß√£o para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        # For better performance in Docker or low-memory environments:\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sess√£o em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                # Exemplo: verificar o tamanho do markdown\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Fun√ß√£o principal\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "# Executar a fun√ß√£o principal\n",
        "#asyncio.run(main()) # <-- LINHA que poss√≠velmente deu erro\n",
        "await main()"
      ],
      "metadata": {
        "id": "NmjYtDBjMxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "-hUDO9qINOWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/'Projetos Python - Google Colab (Colab Notebooks)'/Crawl4AI/Pydantic"
      ],
      "metadata": {
        "id": "n-j8Gl9wWJ34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/arquivos.zip /content/drive/MyDrive/pasta_com_seus_arquivos"
      ],
      "metadata": {
        "id": "p87C1qBJTnWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Drive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic"
      ],
      "metadata": {
        "id": "_qdwiSzOUk-X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkCJw-fSavwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2 - Modifica√ß√£o para baixar .md no diret√≥rio \"/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic\""
      ],
      "metadata": {
        "id": "w9EpU6Saa4SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U crawl4ai"
      ],
      "metadata": {
        "id": "SQGAzDPaeK36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar depend√™ncias\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# Definir o diret√≥rio de sa√≠da\n",
        "output_directory = \"/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic\"\n",
        "\n",
        "# Fun√ß√£o para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        # For better performance in Docker or low-memory environments:\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator(),\n",
        "        output_dir=output_directory  # Configurar o diret√≥rio de sa√≠da aqui\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sess√£o em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                # Exemplo: verificar o tamanho do markdown\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Fun√ß√£o principal\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "# Executar a fun√ß√£o principal (usando await no Colab)\n",
        "await main()"
      ],
      "metadata": {
        "id": "qElxep7la3aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar depend√™ncias\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# Definir o diret√≥rio de sa√≠da\n",
        "output_directory = \"/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic\"\n",
        "\n",
        "# Fun√ß√£o para rastreamento sequencial (modificada)\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Remova output_dir daqui\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sess√£o em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo manualmente\n",
        "                filename = os.path.join(output_directory, f\"{url.replace('/', '_')}.md\") # Nome do arquivo baseado na URL\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True) # Cria o diret√≥rio se necess√°rio\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Fun√ß√£o principal\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "# Executar a fun√ß√£o principal (usando await no Colab)\n",
        "await main()"
      ],
      "metadata": {
        "id": "mN-Oqe1-fMDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V3 - üî• Faz o Crawler e baixa diretamente no Desktop üî•\n",
        "OBS.: Eu estava colocando pra salvar no Drive mas a a vers√£o do crawl4ai (0.4.247) que voc√™ est√° usando n√£o suporta o argumento output_dir no construtor CrawlerRunConfig.\n",
        "\n",
        "**Explica√ß√µes:**\n",
        "\n",
        "1. **`temp_output_directory = \"/content/crawl4ai_temp_output\"`:** Define um diret√≥rio tempor√°rio dentro do ambiente do Colab para armazenar os arquivos Markdown.\n",
        "2. **`crawl_sequential` (modificada):**\n",
        "    *   O `output_dir` foi removido do `CrawlerRunConfig`.\n",
        "    *   Os arquivos s√£o salvos temporariamente no `temp_output_directory` usando a l√≥gica que discutimos anteriormente (`os.makedirs`, `with open...`).\n",
        "3. **`shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)`:**\n",
        "    *   Ap√≥s o rastreamento, esta linha usa a fun√ß√£o `make_archive` do m√≥dulo `shutil` para compactar o diret√≥rio tempor√°rio em um arquivo chamado `crawl4ai_output.zip`.\n",
        "4. **`files.download(\"crawl4ai_output.zip\")`:** Esta linha usa a fun√ß√£o `download` do `google.colab.files` para disponibilizar o arquivo zipado para download no seu navegador.\n",
        "\n",
        "**Como executar:**\n",
        "\n",
        "1. Execute o c√≥digo.\n",
        "2. O c√≥digo ir√° rastrear as URLs e salvar os arquivos Markdown temporariamente.\n",
        "3. Ao final da execu√ß√£o, o arquivo `crawl4ai_output.zip` ser√° baixado automaticamente pelo seu navegador.\n",
        "4. Descompacte o arquivo `.zip` para acessar os arquivos Markdown.\n",
        "\n",
        "**Limpeza (opcional):**\n",
        "\n",
        "*   Voc√™ pode adicionar o seguinte c√≥digo ao final da fun√ß√£o `main()` para remover o diret√≥rio tempor√°rio ap√≥s o download, se desejar:\n",
        "\n",
        "```python\n",
        "shutil.rmtree(temp_output_directory)\n",
        "print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "```\n",
        "\n",
        "Esta solu√ß√£o alternativa permite que voc√™ baixe os arquivos gerados pelo `crawl4ai` como um arquivo zipado, mesmo que esteja enfrentando problemas com o argumento `output_dir`.\n"
      ],
      "metadata": {
        "id": "RmB4YkEohVw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar depend√™ncias\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Definir o diret√≥rio tempor√°rio para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Fun√ß√£o para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sess√£o em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Fun√ß√£o principal - ‚ö†Ô∏èINSIRA AQUI AS URLs para Crawler‚ö†Ô∏è\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "    # Compactar a pasta de sa√≠da\n",
        "    shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "    print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "    # Disponibilizar para download\n",
        "    files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "# Executar a fun√ß√£o principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "BaZLOsp0hWrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V3.1 - üî• Faz o Crawler e baixa diretamente no Desktop üî• (COM SEPARA√á√ÉO DE C√âLULAS)\n",
        "OBS.: Eu estava colocando pra salvar no Drive mas a a vers√£o do crawl4ai (0.4.247) que voc√™ est√° usando n√£o suporta o argumento output_dir no construtor CrawlerRunConfig.\n",
        "\n",
        "**Explica√ß√µes:**\n",
        "\n",
        "1. **`temp_output_directory = \"/content/crawl4ai_temp_output\"`:** Define um diret√≥rio tempor√°rio dentro do ambiente do Colab para armazenar os arquivos Markdown.\n",
        "2. **`crawl_sequential` (modificada):**\n",
        "    *   O `output_dir` foi removido do `CrawlerRunConfig`.\n",
        "    *   Os arquivos s√£o salvos temporariamente no `temp_output_directory` usando a l√≥gica que discutimos anteriormente (`os.makedirs`, `with open...`).\n",
        "3. **`shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)`:**\n",
        "    *   Ap√≥s o rastreamento, esta linha usa a fun√ß√£o `make_archive` do m√≥dulo `shutil` para compactar o diret√≥rio tempor√°rio em um arquivo chamado `crawl4ai_output.zip`.\n",
        "4. **`files.download(\"crawl4ai_output.zip\")`:** Esta linha usa a fun√ß√£o `download` do `google.colab.files` para disponibilizar o arquivo zipado para download no seu navegador.\n",
        "\n",
        "**Como executar:**\n",
        "\n",
        "1. Execute o c√≥digo.\n",
        "2. O c√≥digo ir√° rastrear as URLs e salvar os arquivos Markdown temporariamente.\n",
        "3. Ao final da execu√ß√£o, o arquivo `crawl4ai_output.zip` ser√° baixado automaticamente pelo seu navegador.\n",
        "4. Descompacte o arquivo `.zip` para acessar os arquivos Markdown.\n",
        "\n",
        "üìå**Fluxo de trabalho recomendado:** (üìåNOVIDADE v3.1üìå)\n",
        "\n",
        "1. **Execute a c√©lula de depend√™ncias uma vez** no in√≠cio ou sempre que precisar instalar ou atualizar uma biblioteca.\n",
        "2. **Reinicie o ambiente de execu√ß√£o (Runtime -> Restart runtime)** ap√≥s instalar as depend√™ncias (especialmente se voc√™ atualizou uma biblioteca). Isso garante que as novas vers√µes das bibliotecas sejam carregadas corretamente.\n",
        "3. **Execute as c√©lulas de c√≥digo principal** sempre que fizer altera√ß√µes no c√≥digo e precisar test√°-las. Voc√™ n√£o precisa executar a c√©lula de depend√™ncias novamente, a menos que precise alterar as bibliotecas instaladas.\n",
        "\n",
        "**Limpeza (opcional):**\n",
        "\n",
        "*   Voc√™ pode adicionar o seguinte c√≥digo ao final da fun√ß√£o `main()` para remover o diret√≥rio tempor√°rio ap√≥s o download, se desejar:\n",
        "\n",
        "```python\n",
        "shutil.rmtree(temp_output_directory)\n",
        "print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "```\n",
        "\n",
        "Esta solu√ß√£o alternativa permite que voc√™ baixe os arquivos gerados pelo `crawl4ai` como um arquivo zipado, mesmo que esteja enfrentando problemas com o argumento `output_dir`.\n"
      ],
      "metadata": {
        "id": "eFAKNjlhj-Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. C√©lula de depend√™ncias:"
      ],
      "metadata": {
        "id": "QHKIw3n3kKUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar depend√™ncias (execute esta c√©lula apenas uma vez ou quando precisar atualizar as bibliotecas)\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "GzghpzHLkIDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. C√©lulas de c√≥digo principal:"
      ],
      "metadata": {
        "id": "_SFuoJMXkO2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas (execute esta c√©lula sempre que reiniciar o ambiente de execu√ß√£o)\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Definir o diret√≥rio tempor√°rio para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# ... (o restante do seu c√≥digo)"
      ],
      "metadata": {
        "id": "wTGWYzcDkVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fluxo de trabalho recomendado:"
      ],
      "metadata": {
        "id": "zwMavkwvkRk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sess√£o em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem conclu√≠das, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Fun√ß√£o principal - ‚ö†Ô∏èINSIRA AQUI AS URLs para Crawler‚ö†Ô∏è\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "    # Compactar a pasta de sa√≠da\n",
        "    shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "    print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "    # Disponibilizar para download\n",
        "    files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "# Executar a fun√ß√£o principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "2MtTZyC1kVjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}