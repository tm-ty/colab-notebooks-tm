{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY3a+4oShMriRLqXzoxlOR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tm-ty/colab-notebooks-tm/blob/main/Crawler_Manual_(Crawl4AI)_v3_1_Working_(helper_Gemini_exp_1206).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v1"
      ],
      "metadata": {
        "id": "NjXU3bXpLlTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## passo 1"
      ],
      "metadata": {
        "id": "AlGVmezfLqS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDDMioOuLb-O"
      },
      "outputs": [],
      "source": [
        "!pip install -U crawl4ai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-setup"
      ],
      "metadata": {
        "id": "Y0BnBvpuLenM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!crawl4ai-doctor"
      ],
      "metadata": {
        "id": "CtZP0dcJLf5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "gYQDslcWLffA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 2: Adaptar o código"
      ],
      "metadata": {
        "id": "oR-iAi_ZLwh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OBS.: As duas linhas baixo não são necessárias no Colab\n",
        "# parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "# sys.path.append(parent_dir)"
      ],
      "metadata": {
        "id": "6oDFYo8eLfwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passo 3: Implantar a solução"
      ],
      "metadata": {
        "id": "2v-0qeFeMt3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In_7xZrwMvlY",
        "outputId": "43e19c1e-386b-40d6-a3d3-6b38e32b0773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo 4: Monitorar e obter resultados"
      ],
      "metadata": {
        "id": "vLpHPaQ_NNT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        # For better performance in Docker or low-memory environments:\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                # Exemplo: verificar o tamanho do markdown\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "# Executar a função principal\n",
        "#asyncio.run(main()) # <-- LINHA que possívelmente deu erro\n",
        "await main()"
      ],
      "metadata": {
        "id": "NmjYtDBjMxT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "-hUDO9qINOWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/'Projetos Python - Google Colab (Colab Notebooks)'/Crawl4AI/Pydantic"
      ],
      "metadata": {
        "id": "n-j8Gl9wWJ34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/arquivos.zip /content/drive/MyDrive/pasta_com_seus_arquivos"
      ],
      "metadata": {
        "id": "p87C1qBJTnWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Drive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic"
      ],
      "metadata": {
        "id": "_qdwiSzOUk-X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkCJw-fSavwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# v2 - Modificação para baixar .md no diretório \"/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic\""
      ],
      "metadata": {
        "id": "w9EpU6Saa4SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U crawl4ai"
      ],
      "metadata": {
        "id": "SQGAzDPaeK36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# Definir o diretório de saída\n",
        "output_directory = \"/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic\"\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        # For better performance in Docker or low-memory environments:\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator(),\n",
        "        output_dir=output_directory  # Configurar o diretório de saída aqui\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                # Exemplo: verificar o tamanho do markdown\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "# Executar a função principal (usando await no Colab)\n",
        "await main()"
      ],
      "metadata": {
        "id": "qElxep7la3aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "\n",
        "# Definir o diretório de saída\n",
        "output_directory = \"/content/drive/MyDrive/Projetos Python - Google Colab (Colab Notebooks)/Crawl4AI/Pydantic\"\n",
        "\n",
        "# Função para rastreamento sequencial (modificada)\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Remova output_dir daqui\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo manualmente\n",
        "                filename = os.path.join(output_directory, f\"{url.replace('/', '_')}.md\") # Nome do arquivo baseado na URL\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True) # Cria o diretório se necessário\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "# Executar a função principal (usando await no Colab)\n",
        "await main()"
      ],
      "metadata": {
        "id": "mN-Oqe1-fMDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V3 - 🔥 Faz o Crawler e baixa diretamente no Desktop 🔥\n",
        "OBS.: Eu estava colocando pra salvar no Drive mas a a versão do crawl4ai (0.4.247) que você está usando não suporta o argumento output_dir no construtor CrawlerRunConfig.\n",
        "\n",
        "**Explicações:**\n",
        "\n",
        "1. **`temp_output_directory = \"/content/crawl4ai_temp_output\"`:** Define um diretório temporário dentro do ambiente do Colab para armazenar os arquivos Markdown.\n",
        "2. **`crawl_sequential` (modificada):**\n",
        "    *   O `output_dir` foi removido do `CrawlerRunConfig`.\n",
        "    *   Os arquivos são salvos temporariamente no `temp_output_directory` usando a lógica que discutimos anteriormente (`os.makedirs`, `with open...`).\n",
        "3. **`shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)`:**\n",
        "    *   Após o rastreamento, esta linha usa a função `make_archive` do módulo `shutil` para compactar o diretório temporário em um arquivo chamado `crawl4ai_output.zip`.\n",
        "4. **`files.download(\"crawl4ai_output.zip\")`:** Esta linha usa a função `download` do `google.colab.files` para disponibilizar o arquivo zipado para download no seu navegador.\n",
        "\n",
        "**Como executar:**\n",
        "\n",
        "1. Execute o código.\n",
        "2. O código irá rastrear as URLs e salvar os arquivos Markdown temporariamente.\n",
        "3. Ao final da execução, o arquivo `crawl4ai_output.zip` será baixado automaticamente pelo seu navegador.\n",
        "4. Descompacte o arquivo `.zip` para acessar os arquivos Markdown.\n",
        "\n",
        "**Limpeza (opcional):**\n",
        "\n",
        "*   Você pode adicionar o seguinte código ao final da função `main()` para remover o diretório temporário após o download, se desejar:\n",
        "\n",
        "```python\n",
        "shutil.rmtree(temp_output_directory)\n",
        "print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "```\n",
        "\n",
        "Esta solução alternativa permite que você baixe os arquivos gerados pelo `crawl4ai` como um arquivo zipado, mesmo que esteja enfrentando problemas com o argumento `output_dir`.\n"
      ],
      "metadata": {
        "id": "RmB4YkEohVw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium\n",
        "\n",
        "# Importar bibliotecas\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal - ⚠️INSIRA AQUI AS URLs para Crawler⚠️\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "    # Compactar a pasta de saída\n",
        "    shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "    print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "    # Disponibilizar para download\n",
        "    files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "BaZLOsp0hWrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# V3.1 - 🔥 Faz o Crawler e baixa diretamente no Desktop 🔥 (COM SEPARAÇÃO DE CÉLULAS)\n",
        "OBS.: Eu estava colocando pra salvar no Drive mas a a versão do crawl4ai (0.4.247) que você está usando não suporta o argumento output_dir no construtor CrawlerRunConfig.\n",
        "\n",
        "**Explicações:**\n",
        "\n",
        "1. **`temp_output_directory = \"/content/crawl4ai_temp_output\"`:** Define um diretório temporário dentro do ambiente do Colab para armazenar os arquivos Markdown.\n",
        "2. **`crawl_sequential` (modificada):**\n",
        "    *   O `output_dir` foi removido do `CrawlerRunConfig`.\n",
        "    *   Os arquivos são salvos temporariamente no `temp_output_directory` usando a lógica que discutimos anteriormente (`os.makedirs`, `with open...`).\n",
        "3. **`shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)`:**\n",
        "    *   Após o rastreamento, esta linha usa a função `make_archive` do módulo `shutil` para compactar o diretório temporário em um arquivo chamado `crawl4ai_output.zip`.\n",
        "4. **`files.download(\"crawl4ai_output.zip\")`:** Esta linha usa a função `download` do `google.colab.files` para disponibilizar o arquivo zipado para download no seu navegador.\n",
        "\n",
        "**Como executar:**\n",
        "\n",
        "1. Execute o código.\n",
        "2. O código irá rastrear as URLs e salvar os arquivos Markdown temporariamente.\n",
        "3. Ao final da execução, o arquivo `crawl4ai_output.zip` será baixado automaticamente pelo seu navegador.\n",
        "4. Descompacte o arquivo `.zip` para acessar os arquivos Markdown.\n",
        "\n",
        "📌**Fluxo de trabalho recomendado:** (📌NOVIDADE v3.1📌)\n",
        "\n",
        "1. **Execute a célula de dependências uma vez** no início ou sempre que precisar instalar ou atualizar uma biblioteca.\n",
        "2. **Reinicie o ambiente de execução (Runtime -> Restart runtime)** após instalar as dependências (especialmente se você atualizou uma biblioteca). Isso garante que as novas versões das bibliotecas sejam carregadas corretamente.\n",
        "3. **Execute as células de código principal** sempre que fizer alterações no código e precisar testá-las. Você não precisa executar a célula de dependências novamente, a menos que precise alterar as bibliotecas instaladas.\n",
        "\n",
        "**Limpeza (opcional):**\n",
        "\n",
        "*   Você pode adicionar o seguinte código ao final da função `main()` para remover o diretório temporário após o download, se desejar:\n",
        "\n",
        "```python\n",
        "shutil.rmtree(temp_output_directory)\n",
        "print(f\"Temporary output directory removed: {temp_output_directory}\")\n",
        "```\n",
        "\n",
        "Esta solução alternativa permite que você baixe os arquivos gerados pelo `crawl4ai` como um arquivo zipado, mesmo que esteja enfrentando problemas com o argumento `output_dir`.\n"
      ],
      "metadata": {
        "id": "eFAKNjlhj-Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Célula de dependências:"
      ],
      "metadata": {
        "id": "QHKIw3n3kKUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências (execute esta célula apenas uma vez ou quando precisar atualizar as bibliotecas)\n",
        "!pip install -U crawl4ai\n",
        "!crawl4ai-setup\n",
        "!crawl4ai-doctor\n",
        "!python -m playwright install --with-deps chromium"
      ],
      "metadata": {
        "id": "GzghpzHLkIDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Células de código principal:"
      ],
      "metadata": {
        "id": "_SFuoJMXkO2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas (execute esta célula sempre que reiniciar o ambiente de execução)\n",
        "import asyncio\n",
        "from typing import List\n",
        "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Definir o diretório temporário para salvar os arquivos\n",
        "temp_output_directory = \"/content/crawl4ai_temp_output\"\n",
        "\n",
        "# ... (o restante do seu código)"
      ],
      "metadata": {
        "id": "wTGWYzcDkVIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Fluxo de trabalho recomendado:"
      ],
      "metadata": {
        "id": "zwMavkwvkRk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para rastreamento sequencial\n",
        "async def crawl_sequential(urls: List[str]):\n",
        "    print(\"\\n=== Sequential Crawling with Session Reuse ===\")\n",
        "\n",
        "    browser_config = BrowserConfig(\n",
        "        headless=True,\n",
        "        extra_args=[\"--disable-gpu\", \"--disable-dev-shm-usage\", \"--no-sandbox\"],\n",
        "    )\n",
        "\n",
        "    crawl_config = CrawlerRunConfig(\n",
        "        markdown_generator=DefaultMarkdownGenerator()\n",
        "        # Sem output_dir\n",
        "    )\n",
        "\n",
        "    # Criar o crawler (abre o navegador)\n",
        "    crawler = AsyncWebCrawler(config=browser_config)\n",
        "    await crawler.start()\n",
        "\n",
        "    try:\n",
        "        session_id = \"session1\"  # Reutilizar a mesma sessão em todas as URLs\n",
        "        for url in urls:\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=crawl_config,\n",
        "                session_id=session_id\n",
        "            )\n",
        "            if result.success:\n",
        "                print(f\"Successfully crawled: {url}\")\n",
        "                print(f\"Markdown length: {len(result.markdown_v2.raw_markdown)}\")\n",
        "\n",
        "                # Salvar o arquivo temporariamente\n",
        "                filename = os.path.join(temp_output_directory, f\"{url.replace('/', '_')}.md\")\n",
        "                os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "                with open(filename, \"w\") as f:\n",
        "                    f.write(result.markdown_v2.raw_markdown)\n",
        "                print(f\"Markdown saved to: {filename}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed: {url} - Error: {result.error_message}\")\n",
        "    finally:\n",
        "        # Depois que todas as URLs forem concluídas, fechar o crawler (e o navegador)\n",
        "        await crawler.close()\n",
        "\n",
        "# Função principal - ⚠️INSIRA AQUI AS URLs para Crawler⚠️\n",
        "async def main():\n",
        "    urls = [\n",
        "        \"https://ai.pydantic.dev/agents/\",\n",
        "        \"https://ai.pydantic.dev/models/\",\n",
        "        \"https://ai.pydantic.dev/dependencies/\",\n",
        "        \"https://ai.pydantic.dev/tools/\"\n",
        "    ]\n",
        "    await crawl_sequential(urls)\n",
        "\n",
        "    # Compactar a pasta de saída\n",
        "    shutil.make_archive(\"crawl4ai_output\", 'zip', temp_output_directory)\n",
        "    print(f\"Output directory zipped to crawl4ai_output.zip\")\n",
        "\n",
        "    # Disponibilizar para download\n",
        "    files.download(\"crawl4ai_output.zip\")\n",
        "\n",
        "# Executar a função principal\n",
        "await main()"
      ],
      "metadata": {
        "id": "2MtTZyC1kVjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}